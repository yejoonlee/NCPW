{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_100_reward_improved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tdFk8mXz_iYe",
        "m_YDiT84EPeM"
      ],
      "authorship_tag": "ABX9TyPiOZrs06hUpARDzhSdXREK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yejoonlee/NCPW/blob/main/NCPW_DL/create_agent/DQN_100_reward_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KaTY1DLmGbk1",
        "outputId": "b6c37e57-0649-4d3f-d6d6-ae6cbe04f227"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_skills = [\n",
        "    {'idx':0,\n",
        "     'name':'kick',\n",
        "     'damage':25,\n",
        "     'cool':2,\n",
        "     'hit_rate':0.9\n",
        "    },\n",
        "    {'idx':1,\n",
        "     'name':'punch',\n",
        "     'damage':10,\n",
        "     'cool':1,\n",
        "     'hit_rate':1.0\n",
        "    }\n",
        "]\n",
        "\n",
        "p_skills = [\n",
        "    {'idx':0,\n",
        "     'name':'kick',\n",
        "     'damage':25,\n",
        "     'cool':2,\n",
        "     'hit_rate':0.9\n",
        "    },\n",
        "    {'idx':1,\n",
        "     'name':'punch',\n",
        "     'damage':10,\n",
        "     'cool':1,\n",
        "     'hit_rate':1.0\n",
        "    },\n",
        "    {'idx':2,\n",
        "     'name':'heal',\n",
        "     'damage':0,\n",
        "     'cool':5,\n",
        "     'hit_rate':1.0\n",
        "    },\n",
        "    {'idx':3,\n",
        "     'name':'jump',\n",
        "     'damage':0,\n",
        "     'cool':1,\n",
        "     'hit_rate':1.0\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "kYKla7DyGi_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import prepare_class\n",
        "def to_tensor(np_array: np.array, size=None) -> torch.tensor:\n",
        "    torch_tensor = torch.from_numpy(np_array).float()\n",
        "    if size is not None:\n",
        "        torch_tensor = torch_tensor.view(size)\n",
        "    return torch_tensor\n",
        "\n",
        "\n",
        "def to_numpy(torch_tensor: torch.tensor) -> np.array:\n",
        "    return torch_tensor.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "class EMAMeter:\n",
        "\n",
        "    def __init__(self,\n",
        "                 alpha: float = 0.5):\n",
        "        self.s = None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, y):\n",
        "        if self.s is None:\n",
        "            self.s = y\n",
        "        else:\n",
        "            self.s = self.alpha * y + (1 - self.alpha) * self.s\n",
        "\n",
        "from random import sample\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        # deque object that we've used for 'episodic_memory' is not suitable for random sampling\n",
        "        # here, we instead use a fix-size array to implement 'buffer'\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def push(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "def soft_update(net, net_target, tau):\n",
        "    for param_target, param in zip(net_target.parameters(), net.parameters()):\n",
        "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "def prepare_training_inputs(sampled_exps, device='cpu'):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "    dones = []\n",
        "    for sampled_exp in sampled_exps:\n",
        "        states.append(sampled_exp[0])\n",
        "        actions.append(sampled_exp[1])\n",
        "        rewards.append(sampled_exp[2])\n",
        "        next_states.append(sampled_exp[3])\n",
        "        dones.append(sampled_exp[4])\n",
        "\n",
        "    states = torch.cat(states, dim=0).float().to(device)\n",
        "    actions = torch.cat(actions, dim=0).to(device)\n",
        "    rewards = torch.cat(rewards, dim=0).float().to(device)\n",
        "    next_states = torch.cat(next_states, dim=0).float().to(device)\n",
        "    dones = torch.cat(dones, dim=0).float().to(device)\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_dim: int,\n",
        "                 action_dim: int,\n",
        "                 qnet: nn.Module,\n",
        "                 qnet_target: nn.Module,\n",
        "                 lr: float,\n",
        "                 gamma: float,\n",
        "                 epsilon: float):\n",
        "        \"\"\"\n",
        "        :param state_dim: input state dimension\n",
        "        :param action_dim: action dimension\n",
        "        :param qnet: main q network\n",
        "        :param qnet_target: target q network\n",
        "        :param lr: learning rate\n",
        "        :param gamma: discount factor of MDP\n",
        "        :param epsilon: E-greedy factor\n",
        "        \"\"\"\n",
        "\n",
        "        super(DQN, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.qnet = qnet\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
        "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
        "\n",
        "        # target network related\n",
        "        qnet_target.load_state_dict(qnet.state_dict())\n",
        "        self.qnet_target = qnet_target\n",
        "        self.criteria = nn.SmoothL1Loss()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        qs = self.qnet(state)\n",
        "        prob = np.random.uniform(0.0, 1.0, 1)\n",
        "        if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
        "            action = np.random.choice(range(self.action_dim))\n",
        "        else:  # greedy\n",
        "            action = qs.argmax(dim=-1)\n",
        "        return int(action)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        s, a, r, ns = state, action, reward, next_state\n",
        "\n",
        "        # compute Q-Learning target with 'target network'\n",
        "        with torch.no_grad():\n",
        "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
        "            q_target = r + self.gamma * q_max * (1 - done)\n",
        "\n",
        "        q_val = self.qnet(s).gather(1, a)\n",
        "        loss = self.criteria(q_val, q_target)\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 output_dim: int,\n",
        "                 num_neurons: list = [64, 32],\n",
        "                 hidden_act: str = 'ReLU',\n",
        "                 out_act: str = 'Identity'):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_neurons = num_neurons\n",
        "        self.hidden_act = getattr(nn, hidden_act)()\n",
        "        self.out_act = getattr(nn, out_act)()\n",
        "\n",
        "        input_dims = [input_dim] + num_neurons\n",
        "        output_dims = num_neurons + [output_dim]\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
        "            is_last = True if i == len(input_dims) - 1 else False\n",
        "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
        "            if is_last:\n",
        "                self.layers.append(self.out_act)\n",
        "            else:\n",
        "                self.layers.append(self.hidden_act)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer(xs)\n",
        "        return xs\n",
        "\n",
        "class boss_raid_simulater():\n",
        "    def __init__(self, b_skills, p_skills, reward_rate):\n",
        "\n",
        "        self.reward_rate = reward_rate\n",
        "\n",
        "        self.full_hp = 100\n",
        "        \n",
        "        self.num_bs = len(b_skills)\n",
        "        self.num_ps = len(p_skills)\n",
        "        \n",
        "        self.state_dict = dict()\n",
        "        self.state_dict['b_hp'] = self.full_hp\n",
        "        for i in range(len(b_skills)):\n",
        "            self.state_dict[f'b_cool_{i}'] = 0\n",
        "        self.b_skills = b_skills\n",
        "        \n",
        "        self.state_dict['p_hp'] = self.full_hp\n",
        "        for i in range(len(p_skills)):\n",
        "            self.state_dict[f'p_cool_{i}'] = 0\n",
        "        self.p_skills = p_skills\n",
        "\n",
        "        self.state_pre_dict = self.state_dict.copy()\n",
        "        self.state = np.array(list(self.state_dict.values()))\n",
        "        \n",
        "        self.boss_action = []\n",
        "        self.done = 0\n",
        "        self.len_t = 0\n",
        "        self.len_st = 1\n",
        "        \n",
        "        self.reward = 0\n",
        "        \n",
        "    def observe(self):\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, p_action):\n",
        "        self.len_t += 1\n",
        "        len_t = self.len_t\n",
        "        p_skill = self.p_skills[int(p_action)]\n",
        "        \n",
        "        b_action = int(np.random.uniform(0.0, 1.0, 1).round())\n",
        "        self.boss_action.append(b_action)\n",
        "        b_skill = self.b_skills[b_action]\n",
        "        \n",
        "#         self.battle(p_skill, b_skill)\n",
        "        self.battle_with_cool(p_skill, b_skill)\n",
        "\n",
        "        if self.len_t > 20:\n",
        "          self.len_st += 0.2\n",
        "\n",
        "        self.reward = - (self.state_dict['b_hp']/self.len_st)*self.reward_rate[0]\\\n",
        "                      + (self.state_dict['p_hp']/self.len_st)*self.reward_rate[1]\n",
        "        \n",
        "        if self.state_dict['b_hp'] <= 0 or self.state_dict['p_hp'] <= 0:\n",
        "            self.done = 1\n",
        "            self.reward -= self.len_t*self.reward_rate[2]\n",
        "            \n",
        "            if self.state_dict['b_hp'] <= 0:\n",
        "                self.reward += self.full_hp*max([self.reward_rate[0],self.reward_rate[1]])*self.reward_rate[3]\n",
        "            elif self.state_dict['p_hp'] <= 0:\n",
        "                self.reward -= self.full_hp*max([self.reward_rate[0],self.reward_rate[1]])*self.reward_rate[3]\n",
        "                \n",
        "        self.state_pre_dict = self.state_dict.copy()\n",
        "        ns = self.observe()\n",
        "        r = self.reward\n",
        "        done = self.done\n",
        "                \n",
        "        return ns, r, done\n",
        "    \n",
        "    def battle_with_cool(self, p_skill, b_skill):\n",
        "        p_d = p_skill['damage']\n",
        "        b_d = b_skill['damage']\n",
        "        \n",
        "        p_c = p_skill['cool']\n",
        "        b_c = b_skill['cool']\n",
        "        \n",
        "        p_i = p_skill['idx']\n",
        "        b_i = b_skill['idx']\n",
        "        \n",
        "        p_n = p_skill['name']\n",
        "        \n",
        "        p_sc = self.state_dict[f'p_cool_{p_i}']\n",
        "        b_sc = self.state_dict[f'b_cool_{b_i}']\n",
        "        \n",
        "        # print(f\"b:{b_skill['name']} / p:{p_skill['name']}\")\n",
        "        \n",
        "        if b_sc == 0:\n",
        "            if p_n == 'jump':\n",
        "                if np.random.uniform(0.0,1.0,1) < 0.7:\n",
        "                    # print('miss')\n",
        "                    pass\n",
        "                else: self.state_dict['p_hp'] -= b_d\n",
        "            else: self.state_dict['p_hp'] -= b_d\n",
        "            self.state_dict[f'b_cool_{b_i}'] = b_c + 1\n",
        "        for i in range(self.num_bs):\n",
        "            self.state_dict[f'b_cool_{i}'] = max([self.state_dict[f'b_cool_{i}'] -1, 0])\n",
        "            \n",
        "        if p_sc == 0:\n",
        "            if p_n == 'heal':\n",
        "                # print('healed')\n",
        "                self.state_dict['p_hp'] += 20\n",
        "            else: self.state_dict['b_hp'] -= p_d\n",
        "            self.state_dict[f'p_cool_{p_i}'] = p_c + 1\n",
        "        for i in range(self.num_ps):\n",
        "            self.state_dict[f'p_cool_{i}'] = max([self.state_dict[f'p_cool_{i}'] -1, 0])\n",
        "            \n",
        "        self.state = np.array(list(self.state_dict.values()))\n",
        "            \n",
        "    # def battle(self, p_skill, b_skill):\n",
        "    #     self.state['b_hp'] -= p_skill['damage']\n",
        "    #     self.state['p_hp'] -= b_skill['damage']"
      ],
      "metadata": {
        "id": "BUH0g_JrGmKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "100퍼센트 승률을 보이는 에이전트 학습 코드.\n",
        "해당 에이전트를 통해서 수행되는 에피소드들의 기록을 log_normal로 정의하여 다른 방식으로 학습된 에이전트들의 행동방식과 비교군으로 사용"
      ],
      "metadata": {
        "id": "RfkAmpRp_AqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4 * 5\n",
        "batch_size = 256\n",
        "gamma = 0.88\n",
        "memory_size = 50000\n",
        "total_eps = 2000\n",
        "eps_max = 0.08\n",
        "eps_min = 0.01\n",
        "sampling_only_until = 2000\n",
        "target_update_interval = 10\n",
        "\n",
        "def get_agent(reward_rate, print_all = False, total_eps = 2000):\n",
        "  env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "  s_dim = env.state.shape[0]\n",
        "  a_dim = len(p_skills)\n",
        "\n",
        "  qnet = MLP(s_dim, a_dim, num_neurons=[64,64])\n",
        "  qnet_target = MLP(s_dim, a_dim, num_neurons=[64,64])\n",
        "\n",
        "  # initialize target network same as the main network.\n",
        "  qnet_target.load_state_dict(qnet.state_dict())\n",
        "  agent = DQN(s_dim, 1, qnet=qnet, qnet_target=qnet_target, lr=lr, gamma=gamma, epsilon=1.0)\n",
        "  memory = ReplayMemory(memory_size)\n",
        "\n",
        "  print_every = 100\n",
        "\n",
        "  for n_epi in range(total_eps):\n",
        "      # epsilon scheduling\n",
        "      # slowly decaying_epsilon\n",
        "      epsilon = max(eps_min, eps_max - eps_min * (n_epi / 200))\n",
        "      agent.epsilon = torch.tensor(epsilon)\n",
        "      env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "      s = env.observe()\n",
        "      cum_r = 0\n",
        "\n",
        "      while True:\n",
        "          s = to_tensor(s, size=(1, s_dim))\n",
        "          a = agent.get_action(s)\n",
        "          ns, r, done = env.step(a)\n",
        "\n",
        "          experience = (s,\n",
        "                        torch.tensor(a).view(1, 1),\n",
        "                        torch.tensor(r / 100.0).view(1, 1),\n",
        "                        torch.tensor(ns).view(1, s_dim),\n",
        "                        torch.tensor(done).view(1, 1))\n",
        "          memory.push(experience)\n",
        "\n",
        "          s = ns\n",
        "          cum_r += r\n",
        "          if done:\n",
        "              break\n",
        "\n",
        "      if len(memory) >= sampling_only_until:\n",
        "          # train agent\n",
        "          sampled_exps = memory.sample(batch_size)\n",
        "          sampled_exps = prepare_training_inputs(sampled_exps)\n",
        "          agent.update(*sampled_exps)\n",
        "\n",
        "      if n_epi % target_update_interval == 0:\n",
        "          qnet_target.load_state_dict(qnet.state_dict())\n",
        "      \n",
        "      if n_epi % print_every == 0:\n",
        "          msg = (n_epi, cum_r, epsilon)\n",
        "          if print_all:\n",
        "            print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
        "\n",
        "  return agent\n",
        "\n",
        "def get_play_log(agent, reward_rate, print_all, n_eps = 100, print_every = 500):\n",
        "\n",
        "  env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "  sum_wr = 0\n",
        "\n",
        "  for _ in range(10):\n",
        "    log = {'states': [],\n",
        "            'p_actions': [],\n",
        "            'b_actions': [],\n",
        "            'rewards': []}\n",
        "\n",
        "    i = 0\n",
        "    for ep in range(n_eps):\n",
        "        env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "        s_dim = env.state.shape[0]\n",
        "        a_dim = len(p_skills)\n",
        "        s = env.observe()\n",
        "        cum_r = 0\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "\n",
        "        while True:\n",
        "            s = to_tensor(s, size=(1, s_dim))\n",
        "            a = agent.get_action(s)\n",
        "            ns, r, done = env.step(a)\n",
        "\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "\n",
        "            s = ns\n",
        "            cum_r += r\n",
        "            if done:\n",
        "                if s[0] < s[3]:\n",
        "                  i += 1\n",
        "                break\n",
        "        \n",
        "        log['states'].append(states)\n",
        "        log['p_actions'].append(actions)\n",
        "        log['rewards'].append(rewards)\n",
        "        log['b_actions'].append(env.boss_action)\n",
        "\n",
        "    if print_all:\n",
        "      print(f'num wins: {i}  /  win rate: {i/100}')\n",
        "    sum_wr += i/100\n",
        "\n",
        "  if print_all:\n",
        "    print(f'win rate avg: {sum_wr/10}')\n",
        "\n",
        "  return log\n",
        "\n",
        "def info_from_log(log):\n",
        "  lens = 0\n",
        "  for s in log['states']:\n",
        "    lens += len(s)\n",
        "  \n",
        "  print(f\"avg len of episode:   {lens/100}\")\n",
        "\n",
        "  sum_phps = 0\n",
        "  for epi in log['states']:\n",
        "    sum_php = 0\n",
        "    for state in epi:\n",
        "      sum_php += state[0][3]\n",
        "    sum_phps += sum_php/len(epi)\n",
        "\n",
        "  print(f\"avg HP of player in episode:   {sum_phps/100}\")\n",
        "\n",
        "  sum_bhps = 0\n",
        "  for epi in log['states']:\n",
        "    sum_bhp = 0\n",
        "    for state in epi:\n",
        "      sum_bhp += state[0][0]\n",
        "    sum_bhps += sum_bhp/len(epi)\n",
        "\n",
        "  print(f\"avg HP of boss in episode:   {sum_bhps/100}\")\n",
        "\n",
        "  rate_dict = dict()\n",
        "  rate_dict['len_epi'] = lens/100\n",
        "  rate_dict['avg_bhp'] = sum_bhps/100\n",
        "  rate_dict['avg_php'] = sum_phps/100\n",
        "  rate_dict['kick'] = 0\n",
        "  rate_dict['punch'] = 0\n",
        "  rate_dict['heal'] = 0\n",
        "  rate_dict['jump'] = 0\n",
        "\n",
        "  for i in range(100):\n",
        "    actions = log['p_actions'][i]\n",
        "    rate_dict['kick'] += actions.count(0)/len(actions)\n",
        "    rate_dict['punch'] += actions.count(1)/len(actions)\n",
        "    rate_dict['heal'] += actions.count(2)/len(actions)\n",
        "    rate_dict['jump'] += actions.count(3)/len(actions)\n",
        "\n",
        "  print(rate_dict)\n",
        "  return rate_dict\n",
        "\n",
        "def create_agent_log(reward_rate, print_all = False):\n",
        "  if print_all:\n",
        "    print(\"============= AGENT TRAINING ==============\\n\")\n",
        "  agent = get_agent(reward_rate, print_all)\n",
        "  if print_all:\n",
        "    print(\"\\n============= PALYING SOME ==============\\n\")\n",
        "  log = get_play_log(agent, reward_rate, print_all)\n",
        "  print(\"\\n============= GET INFO FROM PALYING LOG ==============\\n\")\n",
        "  info = info_from_log(log)\n",
        "\n",
        "  return agent, log, info"
      ],
      "metadata": {
        "id": "7LLckL47Hk2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAFE AGENT\n",
        "자신의 안정성을 더 우선하여 플레이하는 스타일의 에이전트를 학습시키는 것이 목적\n",
        "성능을 그대로 가져가기 위해 전체적인 하이퍼 파라미터들을 되도록 변경하지 않고 reward만 변경하여 구현하고싶지만 맘대로 되진 않겠지"
      ],
      "metadata": {
        "id": "tdFk8mXz_iYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_rate_safe = [1,1.5,0.5]\n",
        "\n",
        "agent_safe, log_safe, info_safe = create_agent_log(reward_rate_safe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "MipHKuEd_6-L",
        "outputId": "720e6f2e-77d3-4d16-882c-62503f2d4f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-775daa2d5b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreward_rate_safe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent_safe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_safe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_safe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_rate_safe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-670dbad76f26>\u001b[0m in \u001b[0;36mcreate_agent_log\u001b[0;34m(reward_rate, print_all)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mprint_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============= AGENT TRAINING ==============\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m   \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mprint_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n============= PALYING SOME ==============\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-670dbad76f26>\u001b[0m in \u001b[0;36mget_agent\u001b[0;34m(reward_rate, print_all)\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m           \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m           experience = (s,\n",
            "\u001b[0;32m<ipython-input-8-446163ec8456>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, p_action)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p_hp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_hp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DPS AGENT\n",
        "생각보다 유지형 플레이어가 잘 나와서 딜러 에이전트를 만들어보자"
      ],
      "metadata": {
        "id": "m_YDiT84EPeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_rate_dps = [1.5,1,1]\n",
        "\n",
        "agent_dps, log_dps, info_dps = create_agent_log(reward_rate_dps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it-5Aj5GFCNn",
        "outputId": "2f244b93-8a44-4360-ebbc-879fe81ef0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= AGENT TRAINING ==============\n",
            "\n",
            "Episode :    0 | Cumulative Reward : -976 | Epsilon : 0.080\n",
            "Episode :  100 | Cumulative Reward : -975 | Epsilon : 0.075\n",
            "Episode :  200 | Cumulative Reward : -118 | Epsilon : 0.070\n",
            "Episode :  300 | Cumulative Reward : -202 | Epsilon : 0.065\n",
            "Episode :  400 | Cumulative Reward :  292 | Epsilon : 0.060\n",
            "Episode :  500 | Cumulative Reward :  227 | Epsilon : 0.055\n",
            "Episode :  600 | Cumulative Reward :  406 | Epsilon : 0.050\n",
            "Episode :  700 | Cumulative Reward :  362 | Epsilon : 0.045\n",
            "Episode :  800 | Cumulative Reward :  478 | Epsilon : 0.040\n",
            "Episode :  900 | Cumulative Reward :  456 | Epsilon : 0.035\n",
            "Episode : 1000 | Cumulative Reward :  448 | Epsilon : 0.030\n",
            "Episode : 1100 | Cumulative Reward :  348 | Epsilon : 0.025\n",
            "Episode : 1200 | Cumulative Reward :  422 | Epsilon : 0.020\n",
            "Episode : 1300 | Cumulative Reward :  352 | Epsilon : 0.015\n",
            "Episode : 1400 | Cumulative Reward :  392 | Epsilon : 0.010\n",
            "Episode : 1500 | Cumulative Reward :  755 | Epsilon : 0.010\n",
            "Episode : 1600 | Cumulative Reward :  372 | Epsilon : 0.010\n",
            "Episode : 1700 | Cumulative Reward :  292 | Epsilon : 0.010\n",
            "Episode : 1800 | Cumulative Reward :  522 | Epsilon : 0.010\n",
            "Episode : 1900 | Cumulative Reward :  388 | Epsilon : 0.010\n",
            "\n",
            "============= PALYING SOME ==============\n",
            "\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "win rate avg: 1.0\n",
            "\n",
            "============= GET INFO FROM PALYING LOG ==============\n",
            "\n",
            "avg len of episode:   10.28\n",
            "avg HP of player in episode:   71.0786361694336\n",
            "{'len_epi': 10.28, 'avg_hp': tensor(71.0786), 'kick': 51.84090909090912, 'punch': 19.8363636363636, 'heal': 28.32272727272726, 'jump': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GjtaFenaFn8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 각 reward rate에 따라 생성된 플레이 결과 비교\n",
        "\n",
        "위의 두 시험과정 이후에 에피소드를 길게 끌면 보상을 많이 얻어 이를 통한 꼼수를 Agent가 학습하는 경향을 없애기 위해 reward_rate의 요소를 4개로 늘리고 reward 조정 방식을 수정함\n",
        "  \n",
        "밸런스형: reward_rate_normal = [1, 1, 2, 2]  \n",
        "방어형: reward_rate_safe = [1, 1.5, 2, 2]  \n",
        "공격형: reward_rate_dps = [2, 1.5, 2, 2]  \n",
        "  \n",
        "  \n",
        "위의 조건으로 학습시켰을 때 아래와 같은 결과가 나왔다.  \n",
        "avg_bhp값이 공격형 Agent에서 가장 작고  \n",
        "avg_php값이 방어형 Agent에서 가장 큰 결과를 보면  \n",
        "\n",
        "reward가 조정된 효과대로 결과가 나온 것으로 보인다."
      ],
      "metadata": {
        "id": "hSQdzdXYNgJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(info_normal)\n",
        "print(info_safe)\n",
        "print(info_dps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yDPsYJlKy-N",
        "outputId": "ea12f55a-ca35-4b1e-92b7-961eb3a4fea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'len_epi': 12.15, 'avg_bhp': tensor(51.5978), 'avg_php': tensor(76.3833), 'kick': 49.28781339433514, 'punch': 0.0, 'heal': 8.928884038666638, 'jump': 41.78330256699822}\n",
            "{'len_epi': 20.78, 'avg_bhp': tensor(59.3858), 'avg_php': tensor(95.0629), 'kick': 23.364255140982767, 'punch': 0.4050171526586621, 'heal': 20.249031941546505, 'jump': 55.98169576481206}\n",
            "{'len_epi': 11.77, 'avg_bhp': tensor(44.9711), 'avg_php': tensor(75.7785), 'kick': 36.26567859885735, 'punch': 24.899297619218192, 'heal': 9.620030474217101, 'jump': 29.214993307707342}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_all = True\n",
        "\n",
        "# reward_rate_normal = [1, 1, 2, 2]\n",
        "# agent_normal, log_normal, info_normal = create_agent_log(reward_rate_normal, print_all)\n",
        "\n",
        "reward_rate_safe = [1, 3, 2, 2]\n",
        "agent_safe, log_safe, info_safe = create_agent_log(reward_rate_safe, print_all)\n",
        "\n",
        "# reward_rate_dps = [2, 1.5, 2, 2]\n",
        "# agent_dps, log_dps, info_dps = create_agent_log(reward_rate_dps, print_all)\n",
        "\n",
        "# reward_rate_speed = [0.1,0.1,10]\n",
        "# agent_speed, log_speed, info_speed = create_agent_log(reward_rate_speed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDL_JXawNwx_",
        "outputId": "ad498c10-262a-4067-b735-61cfc63da837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= AGENT TRAINING ==============\n",
            "\n",
            "Episode :    0 | Cumulative Reward :  -13 | Epsilon : 0.080\n",
            "Episode :  100 | Cumulative Reward :  -60 | Epsilon : 0.075\n",
            "Episode :  200 | Cumulative Reward : -237 | Epsilon : 0.070\n",
            "Episode :  300 | Cumulative Reward : 1625 | Epsilon : 0.065\n",
            "Episode :  400 | Cumulative Reward : 1510 | Epsilon : 0.060\n",
            "Episode :  500 | Cumulative Reward : 2551 | Epsilon : 0.055\n",
            "Episode :  600 | Cumulative Reward : 2413 | Epsilon : 0.050\n",
            "Episode :  700 | Cumulative Reward : 1837 | Epsilon : 0.045\n",
            "Episode :  800 | Cumulative Reward : 1828 | Epsilon : 0.040\n",
            "Episode :  900 | Cumulative Reward : 2638 | Epsilon : 0.035\n",
            "Episode : 1000 | Cumulative Reward : 2353 | Epsilon : 0.030\n",
            "Episode : 1100 | Cumulative Reward : 1843 | Epsilon : 0.025\n",
            "Episode : 1200 | Cumulative Reward : 2173 | Epsilon : 0.020\n",
            "Episode : 1300 | Cumulative Reward : 2413 | Epsilon : 0.015\n",
            "Episode : 1400 | Cumulative Reward : 2308 | Epsilon : 0.010\n",
            "Episode : 1500 | Cumulative Reward : 1858 | Epsilon : 0.010\n",
            "Episode : 1600 | Cumulative Reward : 2188 | Epsilon : 0.010\n",
            "Episode : 1700 | Cumulative Reward : 2233 | Epsilon : 0.010\n",
            "Episode : 1800 | Cumulative Reward : 2368 | Epsilon : 0.010\n",
            "Episode : 1900 | Cumulative Reward : 2711 | Epsilon : 0.010\n",
            "\n",
            "============= PALYING SOME ==============\n",
            "\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "num wins: 100  /  win rate: 1.0\n",
            "win rate avg: 1.0\n",
            "\n",
            "============= GET INFO FROM PALYING LOG ==============\n",
            "\n",
            "avg len of episode:   10.4\n",
            "avg HP of player in episode:   71.28689575195312\n",
            "avg HP of boss in episode:   53.09901809692383\n",
            "{'len_epi': 10.4, 'avg_bhp': tensor(53.0990), 'avg_php': tensor(71.2869), 'kick': 60.047585747585835, 'punch': 4.344272394272394, 'heal': 35.608141858141835, 'jump': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT 저장 및 불러오기"
      ],
      "metadata": {
        "id": "nAyrSHdjc6Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/contents/\")\n",
        "# import sys; sys.path.append('/contents/MyDrive/FASTCAMPUS/강화학습/ReinforcementLearningAtoZ') # add project root to the python path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcQQpOo7a0pg",
        "outputId": "daa1f300-7b1e-44cb-df36-b8af60dd8f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /contents/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /contents/MyDrive/Colab_Notebooks/projects/NCPW/RL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y_RRa_Xa7_9",
        "outputId": "4a3de9e4-fb1b-4527-cf02-fca216e0fd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A2C_BASIC_50.ipynb  'DQN_agent_[1_1_03_1].ptb'     REINFORCE_BASIC_70.ipynb\n",
            " DDPG.ipynb\t     'DQN_agent_[1_14_03_09].ptb'   REINFORCE_SAFE_70.ipynb\n",
            " DQN_100.ipynb\t     'DQN_agent_[14_1_03_09].ptb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(agent_normal.state_dict(), '/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[10_10_20_20].ptb')\n",
        "torch.save(agent_safe.state_dict(), '/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/agent/DQN_agent_[10_30_20_20]_test.ptb')\n",
        "# torch.save(agent_dps.state_dict(), '/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[20_15_20_20].ptb')"
      ],
      "metadata": {
        "id": "frZyuZoqQOfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_agent_to_load(reward_rate, print_all=False, memory_size=1, batch_size=1, ):\n",
        "    env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "    s_dim = env.state.shape[0]\n",
        "    a_dim = len(p_skills)\n",
        "\n",
        "    qnet = MLP(s_dim, a_dim, num_neurons=[64,64])\n",
        "    qnet_target = MLP(s_dim, a_dim, num_neurons=[64,64])\n",
        "\n",
        "    # initialize target network same as the main network.\n",
        "    qnet_target.load_state_dict(qnet.state_dict())\n",
        "    agent = DQN(s_dim, 1, qnet=qnet, qnet_target=qnet_target, lr=1e-4 * 5, gamma=0.88, epsilon=1.0)\n",
        "    memory = ReplayMemory(memory_size)\n",
        "\n",
        "    # epsilon scheduling\n",
        "    # slowly decaying_epsilon\n",
        "    epsilon = 0.8\n",
        "    agent.epsilon = torch.tensor(epsilon)\n",
        "    env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "    s = env.observe()\n",
        "    cum_r = 0\n",
        "\n",
        "    while True:\n",
        "        s = to_tensor(s, size=(1, s_dim))\n",
        "        a = agent.get_action(s)\n",
        "        ns, r, done = env.step(a)\n",
        "\n",
        "        experience = (s,\n",
        "                      torch.tensor(a).view(1, 1),\n",
        "                      torch.tensor(r / 100.0).view(1, 1),\n",
        "                      torch.tensor(ns).view(1, s_dim),\n",
        "                      torch.tensor(done).view(1, 1))\n",
        "        memory.push(experience)\n",
        "\n",
        "        s = ns\n",
        "        cum_r += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # train agent\n",
        "    sampled_exps = memory.sample(batch_size)\n",
        "    sampled_exps = prepare_training_inputs(sampled_exps)\n",
        "    agent.update(*sampled_exps)\n",
        "\n",
        "    qnet_target.load_state_dict(qnet.state_dict())\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def load_agent(agent_name):\n",
        "    reward_rate = [0, 0, 0, 0]\n",
        "    agent_load = get_agent_to_load(reward_rate)\n",
        "    agent_load.load_state_dict(torch.load(agent_name))\n",
        "\n",
        "    return agent_load"
      ],
      "metadata": {
        "id": "XZ3MnJvprvpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_play_log(agent):\n",
        "  reward_rate = [0,0,0,0]\n",
        "  sum_wr = 0\n",
        "\n",
        "  log = {'states': [],\n",
        "          'p_actions': [],\n",
        "          'b_actions': [],}\n",
        "\n",
        "  env = boss_raid_simulater(b_skills, p_skills, reward_rate)\n",
        "  s_dim = env.state.shape[0]\n",
        "  a_dim = len(p_skills)\n",
        "  s = env.observe()\n",
        "  cum_r = 0\n",
        "\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "\n",
        "  while True:\n",
        "      s = to_tensor(s, size=(1, s_dim))\n",
        "      a = agent.get_action(s)\n",
        "      ns, r, done = env.step(a)\n",
        "\n",
        "      states.append(list(to_numpy(s)[0]))\n",
        "      actions.append(a)\n",
        "      rewards.append(r)\n",
        "\n",
        "      s = ns\n",
        "      cum_r += r\n",
        "      if done:\n",
        "          break\n",
        "  \n",
        "  log['states'] = str(states)\n",
        "  log['p_actions'] = str(actions)\n",
        "  log['b_actions'] = str(env.boss_action)\n",
        "\n",
        "  return log"
      ],
      "metadata": {
        "id": "bO_eu2-m2Ox8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_load = load_agent('/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[20_15_20_20].ptb')"
      ],
      "metadata": {
        "id": "NK-t7iEHuGAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_single_play_log(agent_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6TAOVp5cnZ3",
        "outputId": "1e1e3b52-dcbc-4d99-f474-ee5982316d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b_actions': '[0, 1, 1, 1, 1, 1, 0, 1, 0, 1]',\n",
              " 'p_actions': '[0, 2, 1, 0, 3, 3, 0, 1, 0, 0]',\n",
              " 'states': '[[100.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0], [75.0, 2.0, 0.0, 75.0, 2.0, 0.0, 0.0, 0.0], [75.0, 1.0, 1.0, 85.0, 1.0, 0.0, 5.0, 0.0], [65.0, 0.0, 0.0, 85.0, 0.0, 1.0, 4.0, 0.0], [40.0, 0.0, 1.0, 75.0, 2.0, 0.0, 3.0, 0.0], [40.0, 0.0, 0.0, 75.0, 1.0, 0.0, 2.0, 1.0], [40.0, 0.0, 1.0, 75.0, 0.0, 0.0, 1.0, 0.0], [15.0, 2.0, 0.0, 50.0, 2.0, 0.0, 0.0, 0.0], [5.0, 1.0, 1.0, 40.0, 1.0, 1.0, 0.0, 0.0], [5.0, 0.0, 0.0, 40.0, 0.0, 0.0, 0.0, 0.0]]'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info_from_log(get_play_log(agent_load,[0,0,0,0],True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oXue3ROm5Tq",
        "outputId": "d6692501-578e-48f6-a05a-9529251e78ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num wins: 91  /  win rate: 0.91\n",
            "num wins: 97  /  win rate: 0.97\n",
            "num wins: 91  /  win rate: 0.91\n",
            "num wins: 95  /  win rate: 0.95\n",
            "num wins: 96  /  win rate: 0.96\n",
            "num wins: 97  /  win rate: 0.97\n",
            "num wins: 93  /  win rate: 0.93\n",
            "num wins: 96  /  win rate: 0.96\n",
            "num wins: 91  /  win rate: 0.91\n",
            "num wins: 91  /  win rate: 0.91\n",
            "win rate avg: 0.938\n",
            "avg len of episode:   73.1\n",
            "avg HP of player in episode:   75.17560577392578\n",
            "avg HP of boss in episode:   26.963857650756836\n",
            "{'len_epi': 73.1, 'avg_bhp': tensor(26.9639), 'avg_php': tensor(75.1756), 'kick': 8.264146846635132, 'punch': 5.455127759223847, 'heal': 21.92095764838223, 'jump': 64.35976774575873}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_bhp': tensor(26.9639),\n",
              " 'avg_php': tensor(75.1756),\n",
              " 'heal': 21.92095764838223,\n",
              " 'jump': 64.35976774575873,\n",
              " 'kick': 8.264146846635132,\n",
              " 'len_epi': 73.1,\n",
              " 'punch': 5.455127759223847}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_load_a = load_agent('/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[10_10_20_20].ptb')\n",
        "agent_load_b = load_agent('/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[10_15_20_20].ptb')\n",
        "agent_load_c = load_agent('/contents/MyDrive/Colab_Notebooks/projects/NCPW/RL/DQN_agent_[20_15_20_20].ptb')"
      ],
      "metadata": {
        "id": "N_h_Sd6EStqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_result = info_from_log(get_play_log(agent_load_a,[0,0,0,0],False))\n",
        "b_result = info_from_log(get_play_log(agent_load_b,[0,0,0,0],False))\n",
        "c_result = info_from_log(get_play_log(agent_load_c,[0,0,0,0],False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrdBpbGBT5uE",
        "outputId": "2349dc94-001a-4ab2-f52f-44990644996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg len of episode:   11.92\n",
            "avg HP of player in episode:   74.11561584472656\n",
            "avg HP of boss in episode:   52.157413482666016\n",
            "{'len_epi': 11.92, 'avg_bhp': tensor(52.1574), 'avg_php': tensor(74.1156), 'kick': 53.438247899761066, 'punch': 0.0, 'heal': 9.452469935035712, 'jump': 37.109282165203204}\n",
            "avg len of episode:   19.44\n",
            "avg HP of player in episode:   93.72418212890625\n",
            "avg HP of boss in episode:   60.840614318847656\n",
            "{'len_epi': 19.44, 'avg_bhp': tensor(60.8406), 'avg_php': tensor(93.7242), 'kick': 24.8147592002096, 'punch': 0.8808847414762441, 'heal': 20.112249190335852, 'jump': 54.19210686797827}\n",
            "avg len of episode:   12.04\n",
            "avg HP of player in episode:   76.85961151123047\n",
            "avg HP of boss in episode:   45.200618743896484\n",
            "{'len_epi': 12.04, 'avg_bhp': tensor(45.2006), 'avg_php': tensor(76.8596), 'kick': 34.37449421997812, 'punch': 25.90424736754996, 'heal': 9.575498284638137, 'jump': 30.145760127833753}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qgmeUukEeFwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}